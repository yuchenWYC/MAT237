\documentclass[11pt]{article}

% Libraries.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage{float}
\usepackage{esint}

% Property settings.
\MakePerPage{footnote}
\pagestyle{fancy}
\lhead{Notes by YW, TX}

% Commands
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\bx}[0]{\mathbf{x}}
\newcommand{\bv}[0]{\mathbf{v}}
\newcommand{\bw}[0]{\mathbf{w}}
\newcommand{\real}[0]{\mathbb{R}}
\newcommand{\under}[1]{\underline{#1}}
\newcommand{\proof}[0]{\textit{\underline{proof:} }}
\newcommand{\func}[3]{\tb{#1}: {#2} \rightarrow {#3} }
\newcommand{\vx}[0]{\tb{x}}
\newcommand{\vy}[0]{\tb{y}}
\newcommand{\vo}[0]{\tb{0}}


% Attr.
\title{MAT237 Multivariable Calculus \\ Lecture Notes}
\author{Yuchen Wang, Tingfeng Xia}
\date{\today}

\begin{document}
    \maketitle
    \tableofcontents
    \newpage
\section{Taylor's Theorem}
\subsection{Review of Taylor's Theorem in 1 Dimenson}
\paragraph{Definition of Taylor polynomials} Assume $I \subset \real$ is an open interval and that $f: I \rightarrow \real$ is a function of class $C^k$ on $I$.

For a point $a\in I$, the $kth$ order Taylor polynomial of $f$ at $a$ is the unique polynomial of order at most $k$, denoted$P_{a,k}(h)$ such that
\begin{align*}
    f(a) &= P_{a,k}(0)\\
    f'(a) &= P'_{a,k}(0)\\
    \vdots\\
    f^{(k)}(a) &= P_{a,k}^{(k)}(0)
\end{align*}
\begin{align*}
    P_{a,k}(h) &= f(a) + hf'(a) + \frac{h^2}{2}f"(a) +...+\frac{h^k}{k!}f^{(k)}(a)\\
    &= \sum_{j=0}^{k}\frac{h^j}{j!}f^{(j)}(a)
\end{align*}
\paragraph{Remark}
Taylor's Theorem guarantees that $P_{a,k}(h)$ is a very good approximation of $f(a+h)$, and that the quality of the approximation increases as $k$ increases.
\paragraph{Taylor's Theorem in 1D}Assume $I \subset \real$ is an open interval and that $f: I \rightarrow \real$ is a function of class $C^k$ on $I$. For $a \in I$ and $h \in \real$ such that $a+h\in I$, let $P_{a,k}(h)$ denote the $k$th-order Taylor polynomial at $a$ and define the remainder
$$R_{a,k}(h) := f(a+h) - P_{a,k}(h)$$
Then $$\lim_{h\rightarrow0}\frac{R_{a,k}(h)}{h^k} = 0$$

\subsection{Taylor's Theorem in higher dimensions}
Assume $S \subset \real^n$ is an open set and that $f: S \rightarrow \real$ is a function of class $C^k$ on $S$. For a point $a\in S$, the $kth$ order Taylor polynomial of $f$ at $a$ is the unique polynomial of order at most $k$, denoted$P_{a,k}(\tb{h})$ such that
\begin{align*}
    f(\tb{a}) &= P_{\tb{a},k}(\tb{0}) \\
    \partial^\alpha f(\tb{a}) &= \partial^\alpha P_{\tb{a},k}(\tb{0})
\end{align*}
for all partial derivatives of order up to $k$.

\paragraph{Taylor's Theorem in nD}Assume $S \subset \real^n$ is an open interval and that $f: S \rightarrow \real$ is a function of class $C^k$ on $I$. For $a \in S$ and $h \in \real^n$ such that $a+h\in S$, let $P_{a,k}(h)$ denote the $k$th-order Taylor polynomial at $a$ and define the remainder
$$R_{a,k}(h) := f(a+h) - P_{a,k}(h)$$
Then $$\lim_{h\rightarrow0}\frac{R_{a,k}(h)}{|h|^k} = 0$$

\paragraph{A Taylor polynomial formula for k = 2}
\begin{equation*}
P_{\tb{a},2}(\tb{h}) = f(\tb{a}) + \nabla f(\tb{a})\cdot \tb{h} + \frac{1}{2}(H(\tb{a})\tb{h})\cdot \tb{h}
\end{equation*}
where we remember that $\tb{h} = \tb{x} - \tb{a}$ if we want the result in terms of $x,y$. 
%As a result, $$\lim_{h\rightarrow0}\frac{R_{a,2}(h)}{h^2} = 0$$ for $R_{a,2}(h) = f(a+h) - P_{a,2}(h)$

\section{Critical Points}
\paragraph{Definition} A symmetric $n \times n$ matrix A is
\begin{enumerate}
    \item \tb{positive definite} if $\tb{x}^T A \tb{x} > 0$ for all $x \in \real^n \symbol{92} \{\tb{0}\}$
    \item \tb{nonnegative definite} if $\tb{x}^T A \tb{x} \geq 0$ for all $x \in \real^n$
\end{enumerate}
In addition, we say that A is
\begin{enumerate}
    \item \tb{negative definite} if -A is positive definite
    \item \tb{nonpositive definite} if -A is nonnegative definite
\end{enumerate}
A matrix A is \tb{indefinite} if none of the above holds. Equivalently, A is indefinite if there exist $\tb{x, y}\in \real$ such that $\tb{x}^TA\tb{x} < 0 < \tb{y}^TA\tb{y}$
\paragraph{Theorem 1} Assume that A is a symmetric matrix. Then \newline
\begin{enumerate}
    \item A is positive definite $\iff$ all its eigenvalues are positive \newline
$\iff \exists \lambda_1 > 0$ such that $\tb{x}^TA\tb{x} \geq \lambda_1|\tb{x}|^2$ for all $\tb{x} \in \real^n $
    \item A is nonnegative definite $\iff$ all its eigenvalues are nonnegative \newline
    \item A is indefinite $\iff$ A has both positive and negative eigenvalues
\end{enumerate}
\paragraph{Remark} If A is a symmetric matrix then \newline
The smallest eigenvalue of A = $\min_{\{\tb{u}\in \real^n: |\tb{u}| = 1\}} \tb{u}^TA\tb{u}$
\paragraph{Theorem 2} For the matrix $A = \begin{pmatrix}
    \alpha & \beta \\
    \beta & \gamma 
\end{pmatrix}$,
\begin{enumerate}
    \item if $det A < 0,$ then A is indefinite
    \item if $det A > 0$, then
    \subitem if $\alpha > 0$ then A is positive definite
    \subitem if $\alpha < 0$ then A is negative definite
    \item if $det A = 0$ then at least one eigenvalue equals zero.
\end{enumerate}
\paragraph{Definition} A critical point $\tb{a}$ of $C^2$ function $\tb{f}$ is \under{degenerate} if det$(D_\tb{H}(\tb{a})) = 0$
\paragraph{Theorem 3 - first derivative test} If $\tb{f}: S \in \real^n \rightarrow \real$ is differentiable, then every local extremum is a critical point.
\paragraph{Theorem 4 - second derivative test}
\begin{enumerate}
    \item If $f: S \rightarrow \real$ is $C^2$ and \tb{a} is a local minimum point for $f$, then \tb{a} is a critical point of $f$ and $H(\tb{a})$ is nonnegative definite.
    \item If \tb{a} is a critical point and $H(\tb{a})$ is positive definite, then \tb{a} is a local minimum point.
\end{enumerate}
\paragraph{Corollary} Assume that $f$ is $C^2$ and $\nabla f(\tb{a}) = \tb{0}$
\begin{enumerate}
    \item If H(a) is positive definite, then a is a local min;
    \item If H(a) is negative definite, then a is a local max;
    \item If H(a) is indefinite, then a is a saddle point;
    \item If none of the above hold, then we cannot determine the character of the critical point without further thought.
\end{enumerate} 

\paragraph{E.Knight's approach to critical points.}In solving a question of $f:\real^2 \rightarrow{} \real$ we could use the following ``quick check" approach:
\begin{enumerate}
    \item Calculate the gradient of $F$, equating it to zero to find the critical points
    \item Calculate the Hessian of $F$, find the correspnding matrices for each critical points, where the Hessian is defined as
    \begin{equation*} H(f) = 
        \begin{bmatrix}
             \partial_{xx}f & \partial_{xy}f = \partial_{yx}f \\
             \partial_{xy}f = \partial_{yx}f & \partial_{yy}f
        \end{bmatrix}
    \end{equation*}
    \item Calculate the determinant of the hessian, and there are the following cases to consider
    \begin{enumerate}
        \item det$H<0$, then $sig(H) = (1,1)$ and the point is a saddle point
        \item det$H>0$, then
            \begin{enumerate}
                \item $tr(H)<0 \implies sig(H) = (2,0)$ and the point is a local minimum
                \item $tr(H)>0 \implies sig(H) = (0,2)$ and the point is a local maximum
            \end{enumerate}
        \item det$H=0$, then the test is inconclusive. We have to do this case by starring at it.
    \end{enumerate}
\end{enumerate}
\section{Lagrange Multipliers}
\subsection{Two constraints}
Assume that $f, g_1$ and $g_2$ are functions $\real^n \rightarrow \real$ of class $C^1$. Assume also that $\{ \nabla g_1(\tb{x}), \nabla g_2(\tb{x})\}$ are linearly independent at all $\tb{x}$ where $g_1(\tb{x}) = g_2(\tb{x}) = 0$ \newline
Then if $x$ is any solution to the optimization problem, there exists $\lambda_1, \lambda_2 \in \real$ such that the following system of equations is satisfies by $\tb{x}, \lambda_1$ and $\lambda_2$:
\begin{equation*}
\begin{cases}
  \nabla f(\tb{x}) + \lambda_1 \nabla g_1(\tb{x}) + \lambda_2 \nabla g_2(\tb{x}) &= \tb{0} \\
  g_1(\tb{x}) &= 0\\
  g_2(\tb{x}) &= 0
\end{cases}
\end{equation*}
\subsection{Inequality Constraints}
Consider the problem
\begin{equation*}
    \begin{cases}
        \mbox{minimize/maximize } f(\tb{x})\\
        \mbox{subject to the constraint: } g(\tb{x}) \leq 0
    \end{cases}
\end{equation*}
where we assume that $g$ is $C^2$, say, and that $\nabla g(\tb{x} \neq 0$ on the set $\{\tb{x}\in \real^n: g(\tb{x}) = 0\}$ \newline
We can reduce this to problems we already know how to solve. EVT guarantees that the problem has a solution.
\paragraph{Case 1} The max or min occurs in the set $\{x\in \real^n: g(\tb{x}) < 0$ \newline
Then it is a critical point, which we know how to find and classify
\paragraph{Case 2} The max or min occurs in the set $\{x\in \real^n: g(\tb{x}) = 0$ \newline
Then we can find it by the Lagrange Multiplier technique. \newline
Finally we can choose the smallest/largest value of $f$ and the point where that value is attained from among all the candidates found in steps 1 and 2 above.
\section{The Implicit Function Theorem}
Assume that S is an open subset of $\real^{n+k}$ and that $F: S \rightarrow \real^k$ is a function of class $C^1$. Assume also that $(\tb{a}, \tb{b})$ is a point in S such that $\tb{F(a, b)=0}$ and det$D_{\tb{y}}\tb{F(a, b)} \neq 0$ \newline
1. Then there exists $r_0, r_1 > 0$ such that for every $\tb{x} \in \real^n$ such that $|\tb{x} - \tb{a}| < r_0$, there exists a unique $\tb{y} \in \real^k$ such that $|\tb{y} - \tb{b}| < r_1$
    $$\tb{F(x, y) = 0} (1)$$
    In other words, equation (1) implicitly defines a function $\tb{y = f(x)}$ for $x \in \real^n$ near \tb{a}, with \tb{y = f(x)} close to \tb{b}. Note in particular that \tb{b = f(a)}. \newline
2. Moreover, the function $\tb{f}: B(r_0,\tb{a}) \rightarrow B(r_1, \tb{b}) \subset \real^k$ from part (1) above is of class $C^1$, and its derivatives may be determined by differentiating the identity $$\tb{F(x,f(x)) = 0}$$ and solving to find the partial derivatives of \tb{f}.
\paragraph{Remark} $$D\tb{f(a)} = -[D_\tb{y}\tb{F(a, b)}]^{-1}D_\tb{x}\tb{F(a, b)}$$
\section{The Inverse Function Theorem} Let U and V be open sets in $\real^n$, and assume that $\func{f}{U}{V}$ is a mapping of class $C^1$. \newline
Assume that \tb{a} $\in U$ is a point such that $D\tb{f(a)}$ is invertible. \newline
and let $\tb{b} := \tb{f(a)}$. Then there exist open sets $M \subset U$ and $N \subset V$ such that
\begin{enumerate}
    \item $\tb{a} \in M$ and $\tb{b} \in N$
    \item $\tb{f}$ is one-to-one from M onto N (hence invertible), and
    \item the inverse function $f^{-1}: N \rightarrow M$ is of class $C^1$
\end{enumerate}
Moreover, if $x \in M$ and $y = \tb{f(x)}\in N$, then $$D(\tb{f}^{-1})(\tb{y}) = [D\tb{f(x)}]^{-1}$$
In particular, $$D(\tb{f}^{-1})(\tb{b}) = [D\tb{f(a)}]^{-1}$$

\section{Some Important Coordinate Systems}
\subsection{Polar Coordinates in $\real^2$}
$$\begin{pmatrix}
    x\\y
\end{pmatrix}
= \begin{pmatrix}
    r\cos{\theta}\\
    r\sin{\theta}
\end{pmatrix}
= \tb{f}(r, \theta) $$
For \tb{f} to be a bijection between open sets, we have to restrict its domain and range. A common choice is to specify that \tb{f} is a function $U \rightarrow V$ where
$$U := \{(r, \theta): r > 0, |\theta| < \pi\},   V:= \real^2 \symbol{92}\{(x, 0): x \leq 0 \}$$
(Note that there is a half of the x-axis missing)
\subsection{Spherical Coordinates in $\real^3$}
$$\begin{pmatrix}
    x\\y\\z
\end{pmatrix}
= \begin{pmatrix}
    r\cos{\theta}\sin{\varphi}\\
    r\sin{\theta}\sin{\varphi}\\
    r\cos{\varphi}
\end{pmatrix}
= \tb{f}(r, \theta, \varphi)$$
If we want \tb{f} to be a bijection between open sets U and V, it is necessary to restrict the domain and range in some appropriate way.

\subsection{Cylindrical Coordinates in $\real^3$}
$$\begin{pmatrix}
    x\\y\\z
\end{pmatrix}
= \begin{pmatrix}
    r\cos{\theta}\\
    r\sin{\theta}\\
    z
\end{pmatrix}
= \tb{f}(r, \theta, z)$$

\section{k-Dimensional Manifolds in $\real^n$}
\subsection{The General Case}
Fix $k < n$. For a k-dimensional manifold $M$ in $\real^n$, we say that $M$ has "degrees of freedom" $k$. There are 3 natural ways to represent $M$ (be careful with the dimensions!!! ):
\paragraph{1. As a \tb{graph}:}
        $$\func{f}{U \subset \real^k}{\real^{n-k}}$$ where U is open.
        $$ S = \{(\tb{x}, \tb{f}(\tb{x})) \in \real^n: \tb{x} = \tb{f}(\tb{x}), \forall \tb{x} \in U \}$$
\paragraph{2. As a \tb{level set}:}
    $$ \func{F}{U\in \real^n}{\real^{n-k}}$$ where U is open.
    $$ S = \{\tb{x} \in U: \tb{F}(\tb{x}) = \tb{c}\}$$ for some $\tb{c} \in \real^{n-k}$.\newline
    This is also called the \ti{"zero locus"} of \tb{F} when $\tb{c} = \tb{0}$
\paragraph{Remark 1} The regularity conditions that guarantees that S is smooth is that
\begin{enumerate}
    \item $\nabla F_1(\tb{x}), ..., \nabla F_{n-k}(\tb{x})$ are linearly independent at each $\tb{x} \in S$. Or equivalently,
    \item the matrix $D\tb{F}(\tb{x})$ has rank $n-k$ at every $\tb{x} \in S$.
\end{enumerate}
\paragraph{Remark 2} It can happen that the above conditions are satisfied but S is not smooth. Example: The square of a smooth F, c = 0
\paragraph{3. Parametrically}
$$\func{f}{U \subset \real^k}{\real^{n}}$$ where U is open.
$$ S = \{\tb{f}(\tb{u}): \tb{u} \in U\}$$
\paragraph{Remark} The regularity conditions that guarantees that S is smooth is that
\begin{enumerate}
    \item $\partial_{u_1}\tb{f}(\tb{u}),...,\partial_{u_k}\tb{f}(\tb{u})$ are linearly independent at each $\tb{u} \in U$. Or equivalently,
    \item the matrix $D\tb{f}(\tb{u})$ has rank $k$ at every $\tb{u} \in U$.
\end{enumerate}
\paragraph{Notes} We can prove that if the above conditions are satisfied, then S is smooth. Construct $\func{F}{\real^{2k}}{\real^k}$, then use IFT (the proof is hard but worthwhile to think about since the general case implies every specific case).

\subsection{The Specific Cases}
\paragraph{Theorem 1 - When is a curve regular?}
Assume that $\func{F}{\real^2}{\real}$ is $C^1$, and let
$$S:=\{\tb{x} \in \real^2: F(\tb{x}) = 0 \}$$
If $\tb{a} \in S$ and $\nabla F(\tb{a}) \neq 0$, then there exists some $r>0$ such that $B(r, \tb{a}) \cap S$ is a $C^1$ graph. \newline
(Prove directly using IFT)
\paragraph{Theorem 2 - When is the parametrization regular?} 
Assume that $\func{f}{(a, b)}{\real ^2}$ is $C^1$, and let
$$S:=\{ \tb{f}(t): t \in (a, b)\}$$
If $\tb{f}'(c) \neq 0$ for some $c \in (a, b)$, then there exists some $r>0$ such that $\{ \tb{f} (t): |t-c| < r\}$ is a $C^1$ graph. \newline
\paragraph{Remark}
It says only that the parametrization is regular near $t = c$, it does not say that S is regular near \tb{f}(c). What it means is that when increasing/decreasing t, we have no control over the path of $f(t)$.
\paragraph{Theorem 3- When is a surface regular?}
conditions: $\tb{a} \in S$ and $\nabla F(\tb{a}) \neq 0$
\paragraph{Theorem 4 - When is the parametrization regular?}
conditions: $D\tb{f}(\tb{c})$ has rank 2 at some $c$

\subsection{Remarks on smoothness of a parametric smooth curve} 
\paragraph{Definition.}If $I\subseteq \real$ is an interval, a $\mathcal{C}^1$ map $\gamma: I \rightarrow{} \real^2$ is said to be
\begin{enumerate}
    \item A \textit{regular curve} if $\gamma'(t)\neq 0, \forall t\in I$
    \item A \textit{simple curve} if $\gamma$ is injective on the interior of $I$.
\end{enumerate}
Hence if $\gamma$ is regular, then there is a neighbourhood if each point whose image looks like a graph of a $\mathcal{C}^1$-function. Simplicity guarantees that no funny overlaps can happen, and this is what is need for the curve to be smooth. \textbf{As a conclusion, we say a parametric curve is smooth if and only if it is \textit{regular} and \textit{simple}}. Equivalently we could also convert such parametrization into a zero locus of a $F$ to use the good old method of gradient directly.

\section{Integration}
\subsection{Zero content}
\paragraph{Zero content in 1-D} A set $S\subset \real$ is said to have zero content if
\begin{equation*}
    \forall \epsilon > 0, \exists~\text{intervals}~I_1,...,I_n ~s.t.~ S\subseteq \bigcup_{i=1}^{n}I_i \wedge \sum_{i=1}^{n}{Len(I_i)} < \epsilon
\end{equation*}

\paragraph{Multidimensional zero content.} A set $S\subset \real^n$ is said to have zero content if
\begin{equation*}
    \forall \epsilon > 0, \exists~\text{boxes}~B_1,...,B_n~s.t.~S\subseteq \bigcup_{i=1}^{n}B_i \wedge \sum_{i=1}^{n}{Area(B_i)} < \epsilon
\end{equation*}

\paragraph{Consequence of zero content.}If a set $Z$ has zero content, then
\begin{equation*}
    \forall \epsilon>0, \exists~\text{boxes}~B_1,...,B_n~s.t.~ S\subseteq \bigcup_{i=1}^{n}B^{int}_i \wedge \sum_{i=1}^{n}{Area(B_i)} < \epsilon
\end{equation*}
Notice the extra $int$.

\paragraph{Proposition on zero content} 
\begin{enumerate}
    \item If $Z\subset \real^2$ has zero content and $U\subset Z$, then $U$ has zero content.
    \item If $Z_1,...,Z_k$ have zero content, then so does $\bigcup_1^k Z_j$
    \item $\mathbf{f}:(a_0, b_0) \xrightarrow{} \real^2$ is of class $C_1$, then $\mathbf{f}([a,b])$ has zero content whenever $a_0<a<b<b_0$
\end{enumerate}
\subsection{Theorems of 1-D Integral Calculus}
\paragraph{Lemma: Refined partitions give better approximations} Let $P$ be some partition over an interval and let $P'$ be a refinement of $P$, then
\begin{equation*}
    LS_{P'}f \geq LS_{P}f \wedge US_{P'} \leq US_{P}f
\end{equation*}
Where LS and US stands for lower sum and upper sum respectively.

\paragraph{Lemma: Lower sum is always less then or equal to upper sum} If $P$ and $Q$ are any partitions of $[a,b]$, then $LS_Pf \leq US_Qf$. The essence of this proof is to consider the common refinement of these two partitions.

\paragraph{Lemma. $\epsilon-\delta$ definition of integrability} If $f$ is a bounded function on $[a,b]$, the following conditions are equivalent:
\begin{enumerate}
    \item $f$ is integrable on $[a,b]$
    \item $\forall \epsilon > 0, \exists P$ of $[a,b]$ such that $US_Pf - LS_Pf < \epsilon$
\end{enumerate}

\paragraph{Theorem:  Integration is ``Linear"}
\begin{enumerate}
    \item Suppose $a < b<c$. If $f$ is integrable on $[a,b]$ and on $[b,c]$, then $f$ is integrable on $[a,c]$, further more
    \begin{equation*}
        \int_a^c f(x)dx = \int_a^b f(x)dx + \int_b^c f(x)dx
    \end{equation*}
    \item If $f$ and $g$ are integrable on $[a,b]$, then so is $f+g$, further more
    \begin{equation*}
        \int_a^b [f(x) + g(x)]dx = \int_a^b f(x)dx + \int_a^b g(x)dx
    \end{equation*}
\end{enumerate}

\paragraph{Theorem.} Suppose $f$ is integrable on $[a,b]$.
\begin{enumerate}
    \item If $c\in \mathbb{R}$, the $cf$ is integrable on $[a,b]$, and $\int_a^b cf(x) = c\int_a^bf(x)dx$
    \item Of $[c,d] \subset [a,b]$, then $f$ is integrable on $[c,d]$.
    \item If $g$ is integrable on $[a,b]$ and $f(x) \leq g(x),\forall x \in [a,b]$, then $\int_a^b f(x)dx\leq \int_a^b g(x)dx$
    \item $|f|$ is integrable on $[a,b]$, and $\left|\int_a^bf(x)dx\right| \leq \int_a^b |f(x)|dx$
\end{enumerate}

\paragraph{Theorem: Bounded + monotone $\implies$ integrable} If $f$ is bounded and monotone on $[a,b]$, then $f$ is integrable on $[a,b]$. The proof of this uses the $\epsilon-\delta$ definition of integrability

\paragraph{Theorem: Continuous $\implies$ integrable} If $f$ is continuous on $[a,b]$, then $f$ is integrable on $[a,b]$. Note that continuous is a sufficient but not necessary condition of integrability

\paragraph{Theorem: discontinuous at only finite pts $\implies$ integrable} If $f$ is bounded on $[a,b]$ and continuous at all except finitely many points in $[a,b]$, then $f$ is integrable on $[a,b]$. A easy example of this would be any $\mathbb{R}$ function that has a hole in it.

\paragraph{Theorem: Discontinuous at only zero content $\implies$ integrable} If $f$ is bounded on $[a,b]$ and the set of points in $[a,b]$ at which $f$ is discontinuous has zero content, then $f$ is integrable on $[a,b]$.

\paragraph{Proposition.} Suppose $f$ and $g$ are integrable on $[a,b]$ and $f(x) = g(x)$ for all except finitely many points $x\in [a,b]$. Then $\int_a^bf(x)dx = \int_a^bg(x)dx$. 

\paragraph{The Fundamental Theorem Of Calculus}
\begin{enumerate}
    \item Let $f$ be an integrable function on $[a,b]$. For $x\in [a,b]$, let $F(x) = \int_a^xf(t)dt$. Then $F$ is continuous on $[a,b]$; more-over, $F'(x)$ exists and equals $f(x)$ at every $x$ at which $f$ is continuous,
    \item Let $F$ be a continuous function on $[a,b]$ that is differentiable except perhaps at finitely many points in $[a,b]$, and let $f$ be a function on $[a,b]$ that agrees with $F'$ at all points where the latter is defined. If $f$ is integrable on $[a,b]$, then $\int_a^bf(t)dt=F(b)-F(a)$
\end{enumerate}

\paragraph{Proposition.} Suppose $f$ is integrable on $[a,b]$. Given $\epsilon>0, \exists \delta > 0$ such that if $P= \{x_0,...,x_J\}$ is any partition of $[a,b]$ satisfying
\begin{equation*}
    max\{x_j-x_{j-1} | 1\leq j \leq J\} < \delta
\end{equation*}
the sums $LS_Pf$ and $US_Pf$ differ from $\int_a^bf(x)dx$ by at most $\epsilon$.

\subsection{Generalized Integral Calculus}
\paragraph{Theorems of double integrals}
\begin{enumerate}
    \item If $f_1$ and $f_2$ are integrable on the bounded set $S$ and $c_1,c_2\in \real$, then $c_1f_1 + c_2f_2$ is integrable on $S$, and
    \begin{equation*}
        \iint_S[c_1f_1 + c_2f_2]dA = c_1\iint_Sf_1dA + c_2\iint_Sf_2dA
    \end{equation*}
    
    \item Let $S_1$ and $S_2$ be bounded sets with no points in common (intersection $ =\emptyset$), and let $f$ be a bounded function. If $f$ is integrable on $S_1$ and on $S_2$, then $f$ is integrable on $S_1\cup S_2$, in which case
    \begin{equation*}
        \iint_{S_1\cup S_2}fdA = \iint_{S_1}fdA + \iint_{S_2} fdA
    \end{equation*}
    
    \item If $f$ and $g$ are integrable on $S$ and $f(\mathbf{x})\leq g(\mathbf{x})$ for $\bx \in S$, then $\iint_S fdA \leq \iint_S g dA$
    
    \item If $f$ is integrable on $S$, then so is $|f|$, and
    \begin{equation*}
        \left|\iint_Sf dA\right| \leq \iint_S|f|dA
    \end{equation*}
\end{enumerate}

\paragraph{Theorem.} Suppose $f$ is a bounded function on the rectangle $R$. If the set of points in $R$ at which $f$ is discontinuous has zero content, then f is integrable on $R$.


\paragraph{Discontinuity of characteristic function} The function $\chi_S$ is discontinuous at $\bx$ if and only if $\bx$ is in the boundary of $S$.

\paragraph{Theorem.} Let $S$ be a measurable subset of $\real^2$. Suppose $f:{\real^2} \rightarrow{}{\real}$ is bounded and the set of points in $S$ at which $f$ is discontinuous has zero content. Then $f$ is integrable on $S$. 
\paragraph{Remark on this theorem:} The only points where $f_{\chi_S}$ can be discontinuous are those points in the closure of S where either $f$ or $\chi_S$ is discontinuous. Both of these cases are discontuinity on a set of zero content. And we can definitely fix $S$ inside of a rectangle, then by the previously stated theorem (The theorem directly above), such function is integrable.

\paragraph{Proposition: Integration on a set of zero content evaluates to zero.} Suppose $Z\subset \real^2$ has zero content. If $f:\real^2\rightarrow{} \real$ is bounded, then $f$ is integrable on $Z$ and $\int_Z fdA = 0$

\paragraph{Corollary}
\begin{enumerate}
    \item Suppose that $f$ is integrable on the set $S\subset \real^2$. If $g(\bx) = f(\bx)$ except for $\bx$ in a set of zero content, then $g$ is integrable on $S$ and $\int_S gdA = \int_S fdA$
    \item Suppose that $f$ is integrable on $S$ and on $T$, and $S\cap T$ has zero content. Then $f$ is integrable on $S\cup T$, and $\int_{S\cup T}fdA = \int_Sfd+ \int_TfdA$
\end{enumerate}
\paragraph{Fubini's Theorem} Let $R = \{(x,y): a\leq x\leq b, c \leq y \leq d \}$, and let $f$ be an integrable function on R. Suppose that, for each $y \in [c, d]$, the function $f_y$ defined by $f_y(x) = f(x, y)$ is integrable on $[a,b]$, and the function $g(y) = \int_a^bf(x,y)dx$ is integrable on $[c,d]$. Then
$$\iint_R fdA = \int_c^d\left[\int_a^b f(x,y)dx\right]dy$$
Likewise, if $f^x(y) = f(x,y)$ is integrable on $[c,d]$ for each $x \in [a,b]$, and $h(x) = \int_c^df(x,y)dy$ is integrable on $[a,b]$, then
$$\iint_R fdA = \int_a^b\left[\int_c^d f(x,y)dy\right]dx$$

\subsection{Change of Variables}
\paragraph{Change of Variable formula 1D} If $g$ is a one-to-one function of class $C^1$ on the interval $[a, b]$, then for any continuous function $f$,
$$\int_{[a,b]} f(g(u))|g'(u)|du = \int_{g([a,b])}f(x)dx$$
In practice it is often more convenient to have all the $g$'s on one side of the equation. If we set $I = g([a, b])$, we have $[a, b] = g^{-1}(I)$, and
$$\int_I f(x) fx = \int_{g^{-1}(I)}f(g(u))|g'(u)|du$$
goal: find the analogous formula for multiple integrals. The questions is: How does the volume of a tiny piece of $n$-space change when one applies the transformation G?
\paragraph{Theorem - Change of Variable for linear mappings} Let A be an invertible $n \times n$ matrix, and let $\tb{G}(\tb{u}) = A\tb{u}$ be the corresponding linear transformation of $\real^n$. Suppose S is a measurable region in $\real^n$ and $f$ is an integrable function on S. Then $G^{-1}(S) = \{A^{-1}\tb{x}: \tb{x} \in S\}$ is measurable and $f \circ \tb{G}$ is integrable on $\tb{G}^{-1}(S)$, and
    $$ \int \hdots \int_S f(\vx)d^n\vx = |det A| \int \hdots \int_{\tb{G}^{-1}(S)}f(A\tb{u})d^n\tb{u}$$

\paragraph{Theorem - Change of Variable for general functions} Given open sets U and V in $\real^n$, let $\func{G}{U}{V}$ be a one-to-one transformation of class $C^1$ whose derivative $D\tb{G}(\tb{u})$ is invertible for all $\tb{u} \in U$. Suppose that $T \subset U$ and $S \subset V$ are measurable sets such that $\tb{G}(T) = S$. If $f$ is an integrable function on S, then $f \circ \tb{G}$ is integrable on T, and 
    $$ \int \hdots \int_S f(\vx)d^n\vx = \int \hdots \int_{T}f(\tb{G}(\tb{u}))|\det D\tb{G}(\tb{u})|d^n\tb{u}$$
    
\paragraph{Some important determinants}
\begin{enumerate}
    \item polar coordinates: factor = $r$
    \item cylindrical coordinates: factor = $|\det(Dg)| = r$
    \item spherical coordinates: factor = $r^2\sin\varphi$
\end{enumerate}

\subsection{Functions Defined by Integrals}
\paragraph{Question}  $$ F(\vx) = \int\hdots\int_S f(\vx,\vy)d^n\vy$$ What condition on $f$ guarantee that F behaves well?
\paragraph{Theorem 1 - Continuity of F} Suppose S and T are compact subsets of $\real^n$ and $\real^m$, respectively, and S is measurable. If $f(\vx, \vy)$ is continuous on the set $T \times S = \{(\vx, \vy): \vx \in T, \vy \in S\}$, then the function F defined by $$ F(\vx) = \int\hdots\int_S f(\vx,\vy)d^n\vy$$ is continuous on T.
\paragraph{Theorem 2 - Differentiability of F} Suppose $S \subset \real^n$ is compact and measurable, and $T \subset \real^m$ is open. If f is a continuous function on $T \times S$ that is of class $C^1$ as a function of $\vx \in T$ for each $\vy \in S$, then the function F defined by $$ F(\vx) = \int\hdots\int_S f(\vx,\vy)d^n\vy$$ is of class $C^1$ on T, and 
    $$ \frac{\partial F}{\partial x_j}(\vx) = \int \hdots \int_S \frac{\partial f}{\partial x_j}(\vx, \vy) d^n\vy (\vx \in T)$$
\paragraph{Remark} Situation often occur in which the variable \vx occurs in the limits of integration as well as the integrand. For simplicity we consider the case where $x$ and $y$ are scalar variables: $$F(x) = \int_a^{\varphi(x)} f(x,y)dy (*)$$
We suppose that $f$ is continuous in $x$ and $y$ and of class $C^1$ in $x$ for each $y$, and that $\varphi$ is of class $C^1$. If $f$ does not depend on $x$, the derivative of F can be computed by the fundamental theorem of calculus together with the chain rule:
$$\frac{d}{dx}\int_a^{\varphi(x)}f(y)fy = f(\varphi(x))\varphi'(x)$$
For the more general case (*), we can differentiate F by combining this result with Theorem 2: Differentiate with respect to each $x$ in (*) while treating the others as constants, and add the results
$$F'(x) = f(\varphi(x))\varphi'(x) + \int_a^{\varphi(x)}\frac{\partial f}{\partial x}(x,y) dy$$
\subsection{Improper Multiple Integrals}
There are many situations where one needs to integrate functions over infinite intervals(e.g. half-space or the whole space) or functions that are unbounded near some point in the region of integration. Suppose, for example, that $f$ is a continuous function on $\real^2$ and we wish to define $\iint_{\real^2}fdA$. The obvious idea is to set $$\iint_{\real^2} fdA = \underset{r\rightarrow\infty}{\lim} \iint_{S_r}fdA$$ where the $S_r$'s are a family of measurable sets that fill out $\real^2$ as $r \rightarrow \infty$. For example, we could take $S_r$ to be 
\begin{enumerate}
    \item the disc of radius $r$ about the origin
    \item the square of side length $r$ centered at the origin
    \item the rectangle of side lengths $r$ and $r^2$ centered at the origin
    \item the disc of radius $r$ centered at $(15, -37)$
    \item $\hdots$
\end{enumerate}
No rationale for choosing one over another and no guarantee that different families $S_r$ will yield the same limit.
\paragraph{Proposition} For $p > 0$, define $f_p$ on $\real^n \setminus \{\vo\}$ by $f_p(\vx) = |\vx|^{-p}$. The integral of $f_p$ over a ball $\{\vx:|\vx| < a\}$ is finite if and only if $p<n$; the integral of $f_p$ over the complement of a ball, $\{\vx:|\vx| > a\}$, is finite if and only if $p>n$.
\paragraph{Proposition} $$\int_{-\infty}^\infty e^{-x^2}dx = \sqrt{\pi}$$
\section{Line and Surface Integrals; Vector Analysis}
Let \tb{F} be an $\real^n$-valued function defined on some subset of $\real^n$. We have encountered such things in previous chapters, where we generally thought of them as representing transformations from one region of $\real^n$ to another or coordinate systems on regions of $\real^n$. In this chapter, however, we think of such an \tb{F} as a function that assigns to each point \vx in its domain a vector $\tb{F}(\vx)$, represented pictorially as an arrow based at \vx, and we therefore call it a \tb{vector field}.
\subsection{Arc Length and Line Integrals}
line integrals: integrals over curves\\
based on the idea of cutting up the curve into many tiny pieces, forming appropriate Riemann sums, and passing to the limit
\paragraph{Differentials on Curve; Arc Length} Suppose C is a smooth curve in $\real^n$. We consider two nearby points \vx and $\vx + d\vx$ on the curve; here
$$d\vx = (dx_1,\hdots,dx_n)$$
is the vector difference between the two points, and we imagine it as being infinitely small. We may, however, be more interested in the distance between the two points, traditionally denoted by $ds$, which is $$ds = |d\vx| = \sqrt{dx^2_1+\hdots+d^2x_n}(*)$$
To give these differentials a precise meaning that can be used for calculations, the best procedure is to parametrize the curve. Thus, we assume that C is given by parametric equations $\vx = \tb{g}(t), a\leq t \leq b$, where $g$ is of class $C^1$ and $g'(t) \neq \vo$. Then the neighboring points \vx and $\vx + d\vx$ are given by $\tb{g}(t)$ and $\tb{g}(t+dt)$, so $$d\vx = \tb{g}(t+dt) - \tb{g}(t) = \tb{g}'(t)dt = (\frac{dx_1}{dt},\hdots,\frac{dx_n}{dt})dt$$ Moreover,
$$|d\vx| = |\tb{g}'(t)|dt = \sqrt{(\frac{dx_1}{dt})^2+ \hdots + (\frac{dx_n}{dt})^2}dt$$which is just what one gets by formally multiplying and dividing the expression on the right of (*) by $dt$\\
What happens if we sum up all the infinitesimal increments $d\vx$ or $ds$ - that is, if we integrate the differentials $d\vx$ or $ds = |d\vx|$ over the curve? Integration of the vector increments $d\vx$ just gives the total vector increment, that is, the vector difference between the initial and final points on the curve:
$$\int_C d\vx = \int_a^b \tb{g}'(t)dt = \tb{g}(b)-\tb{g}(a)$$
On the other hand, $ds$ is the straight-line distance between two infinitesimally close points $\vx$ and $\vx + d\vx$ on the curve, and since smooth curves are indistinguishable from their linear approximations on the infinitesimal level, $ds$ is the \ti{arc length} of the bit of curve between $d\vx$ and $\vx + d\vx$. Adding these up gives the total arc length of the curve
$$\mbox{Arc length} = \int_C ds = \int_a^b|\tb{g}'(t)|dt$$
This is the \tb{definition of arc length for a smooth curve}
\paragraph{Notes} The arc length and the vector difference between the two endpoints of the curve should not depend on the particular parametrization we use. The issue here is that a parametrization $\vx = \tb{g}(t)$ determines an \tb{orientation} for the curve C, that is, a determination of which direction along the curve is "forward" (the direction in which the point $\tb{g}(t)$ moves as $t$ increases) and which direction is "backward". 
\paragraph{Definition} The function $\tb{g}: [a,b] \rightarrow \real^n$ is called \tb{piecewise smooth} if
\begin{enumerate}
    \item it is continuous
    \item its derivative exists and is continuous except perhaps at finitely many points $t_j$, at which the one-sided limits $\underset{t\rightarrow t_j\pm}{\lim}\tb{g}'(t)$ exist. 
\end{enumerate}
\paragraph{Line Integrals of Scalar Functions} 
If $f$ is a continuous function whose domain includes a smooth (or piecewise smooth) curve C in $\real^n$, we can integrate $f$ over the curve, taking the differential in the integral to be the element of arc length $ds$. Thus, if C is parametrized by $\vx = \tb{g}(t), a\leq t \leq b$, we define
$$\int_C f ds = \int_a^b f(\tb{g}(t))|\tb{g}'(t)|dt$$ This is independent of the parametrization and the orientation.

\subsection{Green's Theorem}
\paragraph{Piece-wise smooth.}
Let $R\subset \real^2$ be a region such that $R = \overline{R^{int}}$. Assume that $R$ is bounded. We say that $R$ is piece-wise defined smooth if the boundaries satisfy:
\begin{equation*}
    \partial f = \bigcup_{i = 1}^n C_i~\text{with $C_i$ being smooth, $\forall i$}
\end{equation*}

\paragraph{Simple-Closed curve.}
A Jordan Simple-Closed curve is a curve in $\real^2$ that is closed and non-self-overlapping. Such curve divides the region of $\real^2$ into to portions: those that are inside this curve, and those that are outside.

\paragraph{Statement of the Green's Theorem.} Let $C$ be a positively oriented, piece-wise smooth, simple closed curve in a plane, and let $D$ be the region bounded by $C$. If $L(\cdot)$ and $M(\cdot)$ are functions of $(x, y)$ defined on an open region containing $D$ and have continuous partial derivatives there, then
\begin{equation*}
    \ointctrclockwise_C \left(Ldx + Mdy\right) = \iint_D \left(\frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}\right) dxdy
\end{equation*}
where the path of integration along $C$ is anticlockwise.

\end{document}

